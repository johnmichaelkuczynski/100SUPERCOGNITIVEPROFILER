Create a RateLimiter.ts module that manages rate limits for both Claude and OpenAI. Use the logic below to enforce per-minute token caps, request-per-second throttling, and retry-on-failure with exponential backoff.

📁 server/utils/RateLimiter.ts
ts
Copy
Edit
type TokenLogEntry = { tokens: number; timestamp: number };

class RateLimiter {
  private tokenLog: TokenLogEntry[] = [];
  private maxTokensPerMinute: number;
  private maxRequestsPerSecond: number;
  private concurrentLimit: number;
  private activeRequests = 0;
  private lastRequestTime = 0;

  constructor(config: {
    maxTokensPerMinute: number;
    maxRequestsPerSecond: number;
    concurrentLimit: number;
  }) {
    this.maxTokensPerMinute = config.maxTokensPerMinute;
    this.maxRequestsPerSecond = config.maxRequestsPerSecond;
    this.concurrentLimit = config.concurrentLimit;
  }

  private cleanOldTokens() {
    const now = Date.now();
    this.tokenLog = this.tokenLog.filter(entry => now - entry.timestamp < 60_000);
  }

  private tokensUsed(): number {
    this.cleanOldTokens();
    return this.tokenLog.reduce((sum, entry) => sum + entry.tokens, 0);
  }

  private async waitForRequestSlot(): Promise<void> {
    while (
      this.activeRequests >= this.concurrentLimit ||
      Date.now() - this.lastRequestTime < 1000 / this.maxRequestsPerSecond
    ) {
      await new Promise(res => setTimeout(res, 50));
    }
    this.lastRequestTime = Date.now();
    this.activeRequests++;
  }

  private releaseRequestSlot() {
    this.activeRequests = Math.max(0, this.activeRequests - 1);
  }

  async waitUntilAllowed(estimatedTokens: number): Promise<void> {
    await this.waitForRequestSlot();
    while (this.tokensUsed() + estimatedTokens > this.maxTokensPerMinute) {
      await new Promise(res => setTimeout(res, 250));
    }
    this.tokenLog.push({ tokens: estimatedTokens, timestamp: Date.now() });
  }

  async execute<T>(estimatedTokens: number, fn: () => Promise<T>, retries = 3): Promise<T> {
    await this.waitUntilAllowed(estimatedTokens);

    for (let attempt = 0; attempt <= retries; attempt++) {
      try {
        const result = await fn();
        return result;
      } catch (err: any) {
        if (err?.response?.status === 429 || err?.code === 'RateLimitError') {
          const waitTime = 30 * 1000 * (attempt + 1);
          await new Promise(res => setTimeout(res, waitTime));
        } else {
          throw err;
        }
      } finally {
        this.releaseRequestSlot();
      }
    }

    throw new Error("Max retries exceeded for rate-limited request.");
  }
}

// Configure for each LLM
export const ClaudeLimiter = new RateLimiter({
  maxTokensPerMinute: 32000,
  maxRequestsPerSecond: 2,
  concurrentLimit: 3,
});

export const OpenAILimiter = new RateLimiter({
  maxTokensPerMinute: 300000,
  maxRequestsPerSecond: 10,
  concurrentLimit: 5,
});
📁 Example usage in Claude or OpenAI wrapper
ts
Copy
Edit
// Claude usage
import { ClaudeLimiter } from '../utils/RateLimiter';

async function rewriteWithClaude(prompt: string) {
  const estimatedTokens = Math.ceil(prompt.split(" ").length * 1.5);

  return await ClaudeLimiter.execute(estimatedTokens, async () => {
    return await callClaudeAPI(prompt); // your Claude API call here
  });
}

// OpenAI usage
import { OpenAILimiter } from '../utils/RateLimiter';

async function rewriteWithOpenAI(prompt: string) {
  const estimatedTokens = Math.ceil(prompt.split(" ").length * 2); // GPT output is often longer

  return await OpenAILimiter.execute(estimatedTokens, async () => {
    return await callOpenAIAPI(prompt); // your OpenAI API call here
  });
}
